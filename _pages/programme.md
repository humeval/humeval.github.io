---
title: "Workshop Programme"
permalink: /programme/
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## Workshop on Human Evaluation of NLP Systems

### Programme
This is the preliminary workshop programme (All timings are in GMT+1)

| Time          | Event                                |
| ------------- | ------------------------------------ |
| 09:00–09:10 | **Opening**                          |
| 09:10–10:00 | **Invited Talk:** Disagreement in human evaluation: blame the task not the annotators<br>by [Lucia Specia](https://www.imperial.ac.uk/people/l.specia), Imperial College London and University of Sheffield<br><em> It is well known that human evaluators are prone to disagreement and that this is a problem for reliability and reproducibility of evaluation experiments. The reasons for disagreement can fall into two broad categories: (1) human evaluator, including under-trained, under-incentivised, lacking expertise, or ill-intended individuals, e.g., cheaters; and (2) task, including ill-definition, poor guidelines, suboptimal setup, or inherent subjectivity. While in an ideal evaluation experiment many of these elements will be controlled for, I argue that task subjectivity is a much harder issue. In this talk I will cover a number of evaluation experiments on tasks with variable degrees of subjectivity, discuss their levels of disagreement along with other issues, and cover a few practical approaches do address them. I hope this will lead to an open discussion on possible strategies and directions to alleviate this problem.</em>  |
| 10:00–11:00 | **Oral Session 1 (NLG)**             |
| 10:00–10:20 | *It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-enhanced NLG systems* |
| 10:20–10:40 | *Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation* |
| 10:40–11:00 | *Trading Off Diversity and Quality in Natural Language Generation* |
| 11:00–11:30 | *Break*                              |
| 11:30–12:10 | **Oral Session 2 (MT)**              |
| 11:30–11:50 | *Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation* |
| 11:50–12:10 | *Is this translation error critical?: Classification-based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors* |
| 12:10–13:30 | **Poster Session** <br>- Towards objectively evaluating the quality of generated medical summaries <br>- A preliminary study on evaluating Consultation Notes with Post-Editing <br>- The Great Misalignment Problem in Human Evaluation of NLP Methods <br>- A View From The Crowd: Evaluation Challenges for Time-Offset Interaction Applications <br>- Reliability of human evaluation for text summarization: Lessons learned and challenges ahead <br>- On User Interfaces for Large-Scale Document-level Human Evaluation of Machine Translation Outputs <br>- Direct intrinsic evaluation of word embeddings for philosophical text by domain experts <br>- Detecting Post-edited References and Their Effect on Human Evaluation |
| 13:30–15:00 | *Lunch*                              |
| 15:00–15:40 | **Oral Session 3**                   |
| 15:00–15:20 | *A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of NLP Systems using Checklist* |
| 15:20–15:40 | *Interrater disagreement resolution: A systematic procedure to reach consensus in annotation tasks* |
| 15:40–16:40 | **Discussion Panel**                 |
| 16:40–17:00 | *Break*                              |
| 17:00–17:50 | **Invited Talk:** The Ins and Outs of Ethics-Informed Evaluation <br>by [Margaret Mitchell](http://www.m-mitchell.com/), ex-Staff Research Scientist at Google AI<br><em> The modern train/test paradigm in Artificial Intelligence (AI) and Machine Learning (ML) narrows what we can understand about AI models, and skews our understanding of models' robustness in different environments.  In this talk, I will work through the different factors involved in ethics-informed AI evaluation, including connections to ML training and ML fairness, and present an overarching evaluation protocol that addresses a multitude of considerations in developing ethical AI.</em>  |
| 17:50–18:00 | *Closing*                            |

