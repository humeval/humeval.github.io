---
title: "Accepted Papers"
permalink: /accepted-papers/
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## Workshop on Human Evaluation of NLP Systems

### Short Papers

* Trading Off Diversity and Quality in Natural Language Generation
<br />
Hugh Zhang, Daniel Duckworth, Daphne Ippolito and Arvind Neelakantan

* Towards objectively evaluating the quality of generated medical summaries
<br />
Francesco Moramarco, Damir Juric, Aleksandar Savkov and Ehud Reiter

* A preliminary study on evaluating Consultation Notes with Post-Editing
<br />
Francesco Moramarco, Alex Papadopoulos Korfiatis, Aleksandar Savkov and Ehud Reiter

* The Great Misalignment Problem in Human Evaluation of NLP Methods
<br />
Mika Hämäläinen and Khalid Alnajjar

* Direct intrinsic evaluation of word embeddings for philosophical text by domain experts
<br />
Goya van Boven and Jelke Bloem

* Detecting Post-edited References and Their Effect on Human Evaluation
<br />
Věra Kloudová, Ondřej Bojar and Martin Popel


### Long Papers

* It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-enhanced NLG systems
<br />
Miruna-Adriana Clinciu, Dimitra Gkatzia and Saad Mahamood

* Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation
<br />
Jakob Nyberg, Maike Paetzel and Ramesh Manuvinakurike

* Towards a Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation
<br />
Sheila Castilho

* Is this translation error critical?: Classification-based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors
<br />
Katsuhito Sudoh, Kosuke Takahashi and Satoshi Nakamura

* A View From The Crowd: Evaluation Challenges for Time-Offset Interaction Applications
<br />
Alberto Chierici and Nizar Habash

* Reliability of human evaluation for text summarization: Lessons learned and challenges ahead
<br />
Neslihan Iskender, Tim Polzehl and Sebastian Möller

* On User Interfaces for Large-Scale Document-level Human Evaluation of Machine Translation Outputs
<br />
Roman Grundkiewicz, Marcin Junczys-Dowmunt, Christian Federmann and Tom Kocmi

* A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of NLP Systems using Checklist
<br />
Shaily Bhatt, Rahul Jain, Sandipan Dandapat and Sunayana Sitaram

* Interrater disagreement resolution: A systematic procedure to reach consensus in annotation tasks
<br />
Yvette Oortwijn, Thijs Ossenkoppele and Arianna Betti
