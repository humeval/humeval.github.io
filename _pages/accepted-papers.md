---
title: "Accepted Papers"
permalink: /accepted-papers/
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## Workshop on Human Evaluation of NLP Systems

### Short Papers

* [Trading Off Diversity and Quality in Natural Language Generation](../papers/2021.humeval-1.3.pdf)
<br />
Hugh Zhang, Daniel Duckworth, Daphne Ippolito and Arvind Neelakantan

* [Towards Objectively Evaluating the Quality of Generated Medical Summaries](../papers/2021.humeval-1.6.pdf)
<br />
Francesco Moramarco, Damir Juric, Aleksandar Savkov and Ehud Reiter

* [A Preliminary Study on Evaluating Consultation Notes With Post-Editing](../papers/2021.humeval-1.7.pdf)
<br />
Francesco Moramarco, Alex Papadopoulos Korfiatis, Aleksandar Savkov and Ehud Reiter

* [The Great Misalignment Problem in Human Evaluation of NLP Methods](../papers/2021.humeval-1.8.pdf)
<br />
Mika Hämäläinen and Khalid Alnajjar

* [Eliciting Explicit Knowledge From Domain Experts in Direct Intrinsic Evaluation of Word Embeddings for Specialized Domains](../papers/2021.humeval-1.12.pdf)
<br />
Goya van Boven and Jelke Bloem

* [Detecting Post-Edited References and Their Effect on Human Evaluation](../papers/2021.humeval-1.13.pdf)
<br />
Věra Kloudová, Ondřej Bojar and Martin Popel


### Long Papers

* [It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-Enhanced NLG systems](../papers/2021.humeval-1.1.pdf)
<br />
Miruna-Adriana Clinciu, Dimitra Gkatzia and Saad Mahamood

* [Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation](../papers/2021.humeval-1.2.pdf)
<br />
Jakob Nyberg, Maike Paetzel and Ramesh Manuvinakurike

* [Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation](../papers/2021.humeval-1.4.pdf)
<br />
Sheila Castilho

* [Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors](../papers/2021.humeval-1.5.pdf)
<br />
Katsuhito Sudoh, Kosuke Takahashi and Satoshi Nakamura

* [A View From The Crowd: Evaluation Challenges for Time-Offset Interaction Applications](../papers/2021.humeval-1.9.pdf)
<br />
Alberto Chierici and Nizar Habash

* [Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead](../papers/2021.humeval-1.10.pdf)
<br />
Neslihan Iskender, Tim Polzehl and Sebastian Möller

* [On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs](../papers/2021.humeval-1.11.pdf)
<br />
Roman Grundkiewicz, Marcin Junczys-Dowmunt, Christian Federmann and Tom Kocmi

* A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of NLP Systems Using Checklist
<br />
Shaily Bhatt, Rahul Jain, Sandipan Dandapat and Sunayana Sitaram

* Interrater Disagreement Resolution: A Systematic Procedure to Reach Consensus in Annotation Tasks
<br />
Yvette Oortwijn, Thijs Ossenkoppele and Arianna Betti
