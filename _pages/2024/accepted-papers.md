---
title: "Accepted Papers"
permalink: /2024/accepted-papers/
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## Accepted Papers

* [Quality and Quantity of Machine Translation References for Automatic Metrics](https://humeval.github.io/papers/2024.humeval-1.1.pdf)
<br/> Vilém Zouhar and Ondřej Bojar 

* [Exploratory Study on the Impact of English Bias of Generative Large Language Models in Dutch and
French] (https://humeval.github.io/papers/2024.humeval-1.2.pdf)
<br/>
Ayla Rigouts Terryn and Miryam de Lhoneux 

* [Adding Argumentation into Human Evaluation of Long Document Abstractive Summarization: A Case Study on Legal Opinions](https://humeval.github.io/papers/2024.humeval-1.3.pdf)
<br/>
Mohamed Elaraby, Huihui Xu, Morgan Gray, Kevin Ashley and Diane Litman 

* [A Gold Standard with Silver Linings: Scaling Up Annotation for Distinguishing Bosnian, Croatian, Montenegrin and Serbian](https://humeval.github.io/papers/2024.humeval-1.4.pdf)
<br/>
Aleksandra Miletić and Filip Miletić

* [Insights of a Usability Study for KBQA Interactive Semantic Parsing: Generation Yields Benefits over Templates but External Validity Remains Challenging](https://humeval.github.io/papers/2024.humeval-1.5.pdf)
<br/>
Ashley Lewis, Lingbo Mo, Marie-Catherine de Marneffe, Huan Sun and Michael White

* [Extrinsic evaluation of question generation methods with user journey logs](https://humeval.github.io/papers/2024.humeval-1.6.pdf)
<br/>
Elie Antoine, Eléonore Besnehard, Frederic Bechet, Geraldine Damnati, Eric Kergosien and Arnaud
Laborderie 

* [Towards Holistic Human Evaluation of Automatic Text Simplification](https://humeval.github.io/papers/2024.humeval-1.7.pdf)
<br/>
Luisa Carrer, Andreas Säuberli, Martin Kappus and Sarah Ebling 

* [Decoding the Metrics Maze: Navigating the Landscape of Conversational Question Answering System Evaluation in Procedural Tasks](https://humeval.github.io/papers/2024.humeval-1.8.pdf)
<br/>
Alexander Frummet and David Elsweiler 

* [The 2024 ReproNLP Shared Task on Reproducibility of Evaluations in NLP: Overview and Results](https://humeval.github.io/papers/2024.humeval-1.9.pdf)
<br/>
Anya Belz and Craig Thomson 

* [Once Upon a Replication: It is Humans’ Turn to Evaluate AI’s Understanding of Children’s Stories for
QA Generation](https://humeval.github.io/papers/2024.humeval-1.10.pdf)
<br/>
Andra-Maria Florescu, Marius Micluta-Campeanu and Liviu P. Dinu

* [Exploring Reproducibility of Human-Labelled Data for Code-Mixed Sentiment Analysis](https://humeval.github.io/papers/2024.humeval-1.11.pdf)
<br/> 
Sachin Sasidharan Nair, Tanvi Dinkar and Gavin Abercrombie 

* [Reproducing the Metric-Based Evaluation of a Set of Controllable Text Generation Techniques](https://humeval.github.io/papers/2024.humeval-1.12.pdf)
  <br/>
  Michela Lorandi and Anya Belz

* [ReproHum: #0033-03: How Reproducible Are Fluency Ratings of Generated Text? A Reproduction of August et al. 2022](https://humeval.github.io/papers/2024.humeval-1.13.pdf)
  <br/>
  Emiel van Miltenburg, Anouck Braggaar, Nadine Braun, Martijn Goudbeek, Emiel Krahmer, Chris van der Lee, Steffen Pauws and Frédéric Tomas

* [ReproHum #0927-03: DExpert Evaluation? Reproducing Human Judgements of the Fluency of Generated Text](https://humeval.github.io/papers/2024.humeval-1.14.pdf)
  <br/>
  Tanvi Dinkar, Gavin Abercrombie and Verena Rieser

* [ReproHum #0927-3: Reproducing The Human Evaluation Of The DExperts Controlled Text Generation Method](https://humeval.github.io/papers/2024.humeval-1.15.pdf)
  <br/>
  Javier González Corbelle, Ainhoa Vivel Couso, Jose Maria Alonso-Moral and Alberto Bugarín-Diz

* [ReproHum #1018-09: Reproducing Human Evaluations of Redundancy Errors in Data-To-Text Systems](https://humeval.github.io/papers/2024.humeval-1.16.pdf)
  <br/>
 Filip Klubička and John D. Kelleher

* [ReproHum#0043: Human Evaluation Reproducing Language Model as an Annotator: Exploring Dialogue Summarization on AMI Dataset](https://humeval.github.io/papers/2024.humeval-1.17.pdf)
  <br/>
  Vivian Fresen, Mei-Shin Wu-Urbanek and Steffen Eger

* [ReproHum #0712-01: Human Evaluation Reproduction Report for “Hierarchical Sketch Induction for Paraphrase Generation”](https://humeval.github.io/papers/2024.humeval-1.18.pdf)
  <br/>
  Mohammad Arvan and Natalie Parde

* [ReproHum #0712-01: Reproducing Human Evaluation of Meaning Preservation in Paraphrase Generation](https://humeval.github.io/papers/2024.humeval-1.19.pdf)
  <br/>
  Lewis N. Watson and Dimitra Gkatzia

* [ReproHum #0043-4: Evaluating Summarization Models: investigating the impact of education and language proficiency on reproducibility](https://humeval.github.io/papers/2024.humeval-1.20.pdf)
  <br/>
  Mateusz Lango, Patricia Schmidtova, Simone Balloccu and Ondrej Dusek

* [ReproHum #0033-3: Comparable Relative Results with Lower Absolute Values in a Reproduction Study](https://humeval.github.io/papers/2024.humeval-1.21.pdf)
  <br/>
  Yiru Li, Huiyuan Lai, Antonio Toral and Malvina Nissim

* [ReproHum #0124-03: Reproducing Human Evaluations of end-to-end approaches for Referring Expression Generation](https://humeval.github.io/papers/2024.humeval-1.22.pdf)
  <br/>
  Saad Mahamood

* [ReproHum #0087-01: Human Evaluation Reproduction Report for Generating Fact Checking Explanations](https://humeval.github.io/papers/2024.humeval-1.23.pdf)
  <br/>
  Tyler Loakman and Chenghua Lin

* [ReproHum #0892-01: The painful route to consistent results: A reproduction study of human evaluation in NLG](https://humeval.github.io/papers/2024.humeval-1.24.pdf)
  <br/>
  Irene Mondella, Huiyuan Lai and Malvina Nissim

* [ReproHum #0087-01: A Reproduction Study of the Human Evaluation of the Coverage of Fact Checking Explanations](https://humeval.github.io/papers/2024.humeval-1.25.pdf)
  <br/>
  Mingqi Gao, Jie Ruan and Xiaojun Wan

* [ReproHum #0866-04: Another Evaluation of Readers’ Reactions to News Headlines](https://humeval.github.io/papers/2024.humeval-1.26.pdf)
  <br/>
  Zola Mahlaza, Toky Hajatiana Raboanary, Kyle Seakgwa and C. Maria Keet
