---
layout: splash
permalink: /
excerpt: "HumEval at RANLP 2023<br/> Varna, Bulgaria, 7 or 8 September, 2023"
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## The 3rd Workshop on Human Evaluation of NLP Systems (HumEval'23)

## News

**1 March**: The third edition of HumEval will be held at [RANLP 2023](https://ranlp.org/ranlp2023/)! \
**1 May**: Call for Papers published

### Workshop Topic and Content

The HumEval workshops (previously at EACL 2021 and ACL 2022) aim to create a forum for current human evaluation research and future directions, a space for researchers working with human evaluations to exchange ideas and begin to address the issues human evaluation in NLP faces in many respects, including experimental design, meta-evaluation and reproducibility. We invite papers on topics including, but not limited to, the following topics as addressed in any subfield of NLP:

* Experimental design and methods for human evaluations
* Reproducibility of human evaluations
* Inter-evaluator and intra-evaluator agreement
* Ethical considerations in human evaluation of computational systems
* Quality assurance for human evaluation 
* Crowdsourcing for human evaluation
* Issues in meta-evaluation of automatic metrics by correlation with human evaluations
* Alternative forms of meta-evaluation and validation of human evaluations
* Comparability of different human evaluations
* Methods for assessing the quality and the reliability of human evaluations
* Role of human evaluation in the context of Responsible and Accountable AI

We welcome work from any subfield of NLP (and ML/AI more generally), with a particular focus on evaluation of systems that produce language as output.



