---
layout: splash
permalink: /
excerpt: "HumEval at RANLP 2023<br/> Varna, Bulgaria, 7 September, 2023"
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## The 3rd Workshop on Human Evaluation of NLP Systems (HumEval'23)

## News

**1 March**: The third edition of HumEval will be held at [RANLP 2023](https://ranlp.org/ranlp2023/)! \
**1 May**: Call for Papers published \
**15 June**: Elizabeth Clark, Google Research, confirmed as keynote speaker \
**10 July**: Workshop submission deadline extended to 20 July 2023 \
**12 August**: Invited Talk details available

### Invited Speaker: Elizabeth Clark, Google Research

**Title**: The importance (and challenges) of collecting human evaluations for better NLG metrics

**Abstract**: Human evaluations can be used to develop better automatic metrics, both as training data and as meta-evaluation benchmarks for proposed metrics. To support the development of summarization metrics, we released Seahorse, a dataset of 96K multilingual, multifaceted human ratings of summaries. I will describe the dataset and demonstrate how it can be used to train and evaluate summarization metrics. I will then discuss challenges in collecting human evaluations and suggest directions for improving them, especially given the capabilities of todayâ€™s NLG models.

**Bio**: Elizabeth Clark is a research scientist at Google DeepMind in New York. She works on problems in natural language generation and automatic and human evaluation. She received her PhD from the University of Washington, where she worked on models and evaluation for human-machine collaborative writing.

### Workshop Topic and Content

The HumEval workshops (previously at EACL 2021 and ACL 2022) aim to create a forum for current human evaluation research and future directions, a space for researchers working with human evaluations to exchange ideas and begin to address the issues human evaluation in NLP faces in many respects, including experimental design, meta-evaluation and reproducibility. We invite papers on topics including, but not limited to, the following topics as addressed in any subfield of NLP:

* Experimental design and methods for human evaluations
* Reproducibility of human evaluations
* Inter-evaluator and intra-evaluator agreement
* Ethical considerations in human evaluation of computational systems
* Quality assurance for human evaluation 
* Crowdsourcing for human evaluation
* Issues in meta-evaluation of automatic metrics by correlation with human evaluations
* Alternative forms of meta-evaluation and validation of human evaluations
* Comparability of different human evaluations
* Methods for assessing the quality and the reliability of human evaluations
* Role of human evaluation in the context of Responsible and Accountable AI

We welcome work from any subfield of NLP (and ML/AI more generally), with a particular focus on evaluation of systems that produce language as output.



